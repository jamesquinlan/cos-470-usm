%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
% SUMMARY    : Lecture 9
%            : University of Southern Maine 
%            : @james.quinlan
%            : Ellis Fitzgerald --> modified by Mohamed Noor
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\documentclass[12pt]{article}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}
\usepackage{tcolorbox}
\usepackage{geometry}
\geometry{margin=1in}
\usepackage{hyperref}
\usepackage{cite}

\title{Lecture 6: Regression and Loss Functions}
\author{University of Southern Maine}
\date{}

\begin{document}

\maketitle

\section*{Objectives}
\begin{enumerate}
    \item Regression
    \item Loss Functions
    \item Lectures 1-8 Review
    \item Practice Problems
\end{enumerate}

\rule[0.0051in]{\textwidth}{0.00025in}
% ----------------------------------------------------------------

\section{Regression}
The previous lectures and materials have mostly covered classification tasks. As we know, classification can be binary, where there are two (2) labels: 
\[
y \in \{-1, 1\}
\]

Though, classification tasks can contain \(x \in \mathbb{Z}^+\) labels. In a small example, the Iris dataset has three (3) different labels representing the Iris flowers: setosa, versicolor, and virginica.
\[
y \in \{1, 2, 3\}
\]

\textbf{Regression} has the same goal of ``labeling'' features, but not categorically, and instead with real numbers:
\[
y \in \mathbb{R}
\]

For both regression and classification, the dataset takes the same form:
\[
D = \{(x_1, y_1), (x_2, y_2),...,(x_n, y_n)\}
\]

Where for each feature, \(x_i\), there is an associated label, \(y_i\).

\section{Loss Functions}
\textbf{Loss} is calculated by a function for supervised machine learning tasks. Its responsibility is to answer the question: 
\[
\text{By how much is the model \textit{wrong}?}
\]

\subsection{Zero-to-One Loss}
\textbf{Loss} functions can be simple and binary, similar to classification tasks. For example, a zero-to-one loss function would be denoted as:
\[
l_{01} \in \{0, 1\}
\]
In this function, a 1 means the result was incorrect and a 0 means the result matches the anticipated output.

When speaking about loss functions, some may interchangeably call a function's \textbf{Aggregate} (summation) a loss function, but this is more accurately described as being a \textbf{Cost} function:
\[ 
\sum_{i=1}^{n} l_{01}(i) = 1 + 0 + 1 + 1 + 0 . . . 
\]

In this cost function, the higher the value the more loss the model has. Simply put: the higher the score \(\implies\) the higher the error rate. 

\subsection{Absolute Loss}
\textbf{Absolute Loss} takes the actual label associated with the feature \(y_i\) and gets the absolute difference with the predicted label value, \(\hat{y}_i\).
\[
\text{Loss} = |y_i - \hat{y}_i|
\]
\[
\text{Cost} = \sum_{i=1}^{n} |y_i - \hat{y}_i|
\]

The downside of the absolute loss function is that it is not \textbf{differentiable}. Recall that differentiable means a derivative exists.

\subsection{Square Loss}
Square loss takes the actual label associated with the feature \(y_i\) and squares its difference with the computed \(\hat{y}_i\) label value.
\[
\text{Loss} = (y_i - \hat{y}_i)^2
\]
\[
\text{Cost} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
\]

Because this loss function squares its result, it can be sensitive to outliers. However, it is often preferred over the absolute loss function because it is differentiable.

\subsection{Objective Function}
Recall that we covered how a \textbf{Cost} function aggregates a \textbf{Loss} function:
\[
\text{Cost} = \sum_{i=1}^{n} \text{Loss}
\]

Similarly, there is another function known as the \textbf{Objective Function} which is the sum of the cost and a \textbf{Regularizer}:
\[
\text{Objective Function} = \text{Cost} + \text{Regularizer}
\]

The purpose of the regularizer is to discourage large weights by adding a penalty term to the cost function. This helps prevent overfitting and improves generalization. Common regularizers include L1 (Lasso) and L2 (Ridge), which shrink model coefficients during training.

\subsection{Gradient Descent}
Gradient descent \cite{goodfellow2016deep} is essentially a use case for calculating derivatives. In supervised Machine Learning (ML) tasks, its purpose is to find global minimums (where the derivative is 0) to optimize a model's loss function. 

Here are the names of some notable gradient descent methods:
\begin{outline}
    \1 Vanilla
    \1 Stochastic
    \1 Minibatch
    \1 Hessian
    \1 Momentum
\end{outline}

\subsubsection{Learning Rate}
Gradient descent performance depends heavily on the learning rate, which determines the size of steps taken during optimization. A good starting point is often 0.01 or 0.001:
\begin{itemize}
    \item Too large: Overshoots minima.
    \item Too small: Slow convergence.
\end{itemize}

% ----------------------------------------------------------------
\section{Lectures 1-7 Review}

\subsection{The Perceptron\cite{rosenblatt1958perceptron}}
\begin{outline}
    \1 Developed by \textbf{Rosenblatt} in \textbf{1957} at Cornell University
    \1 It was the first AI algorithm
    \1 \textbf{Binary Classification} Task
    \1 The goal is to find a hyperplane to separate the data
    \1 Assumes data is \textbf{Linearly Separable}
    \1 Calculates weight vector \(\vec{w}\) and bias \(b\) 
    \1 ``Bundle \& Save'': This trick will save time by removing the need to calculate \(b\). We do this by ``bundling" \(b\) into \(\vec{w}\): 
    \[
    \vec{w} = 
    \begin{bmatrix}
        w_1 \\
        w_2 \\
        \vdots \\
        w_p \\
        b
    \end{bmatrix}
    \]
    \1 \(\vec{w} \in \mathcal{H}\), where \(\mathcal{H}\) is the \textbf{Decision Boundary} or \textbf{Hyperplane}
\end{outline}

\[
\mathcal{H} = \{\vec{x} \mid \vec{w}^T\vec{x} + b = 0\}
\]

\subsection{KNN\cite{guo2003knn}}
\begin{outline}
    \1 Stands for \textbf{\(k\) Nearest Neighbors}
    \1 Loops over all neighbors and calculates distance
    \1 Sorts distances and returns \(k\) nearest
    \1 Assumes there is a pattern
    \1 Not designed for data with random values
    \1 \textbf{Regression:} Averages values of nearest neighbors
    \1 \textbf{Classification:} Sets label to the mode of nearest neighbor values
\end{outline}

\begin{verbatim}
w = 0
m = 1
while m > 0:
    m = 0
    for i = 1..n:
        if y[i] * dot(w, x[i]) <= 0:
            w = w + alpha * y[i] * x[i]
            m = m + 1
        end
    end
end
\end{verbatim}

% ----------------------------------------------------------------
\section{Practice Problems}

\begin{enumerate}
    \item What is the benefit of having fewer dimensions, features, or components in your vectors?
    \item What are the different types of Supervised ML?
    \item What is a loss function?
    \item What is KNN? How could it be used differently for both Classification and Regression?
    \item What does SVM stand for? What is its purpose?
    \item What is the definition of a distance function? Give examples.
\end{enumerate}

\bibliographystyle{plain}
\bibliography{references}

\end{document}
Footer
